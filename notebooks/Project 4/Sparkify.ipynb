{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import udf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, sum,  asc, avg\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/18 21:00:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x6ced7109) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x6ced7109\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:67)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/Moaze002/Documents/Udacity-DataScience Nanodegree/notebooks/Project 4/Sparkify.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Moaze002/Documents/Udacity-DataScience%20Nanodegree/notebooks/Project%204/Sparkify.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create a Spark session\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Moaze002/Documents/Udacity-DataScience%20Nanodegree/notebooks/Project%204/Sparkify.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Moaze002/Documents/Udacity-DataScience%20Nanodegree/notebooks/Project%204/Sparkify.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m.\u001b[39;49mbuilder\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Moaze002/Documents/Udacity-DataScience%20Nanodegree/notebooks/Project%204/Sparkify.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mSparkify\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Moaze002/Documents/Udacity-DataScience%20Nanodegree/notebooks/Project%204/Sparkify.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    210\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:329\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize_context\u001b[39m(\u001b[39mself\u001b[39m, jconf):\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1573\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1567\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1569\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1570\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1572\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1573\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1574\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1576\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1577\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x6ced7109) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x6ced7109\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:67)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    }
   ],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName('Sparkify')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"/Users/Moaze002/Documents/mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(\"userId\").dropDuplicates().sort(\"userId\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(\"sessionId\").dropDuplicates().sort(\"sessionId\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how = 'any', subset = [\"userId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each column and count duplicates\n",
    "for column in data.columns:\n",
    "    duplicates = data.groupBy(column).agg(count(\"*\").alias(\"count\")).filter(\"count > 1\")\n",
    "    duplicates.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_duplicated = data.select(\"*\").dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag the records where Cancellation Confirmation page is reached - 1 if it is and 0 if not\n",
    "churn_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "#creating churn column\n",
    "data_churn = data_no_duplicated.withColumn(\"Churn\", churn_event(\"page\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = data_churn.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd['Churn'].value_counts().to_frame().style.background_gradient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of records that is omitted becuase of empty id is: {(number_of_rows - data_pd.shape[0])/data_pd.shape[0] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd[data_pd['Churn']==1].userId.count()\n",
    "\n",
    "# Number of churn customers 😞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a density plot\n",
    "plt.hist(data=data_pd, x='length', bins=50, density=True, alpha=0.7\n",
    "        )\n",
    "\n",
    "# add a vertical line at the median length\n",
    "median_length = data_pd['length'].median()\n",
    "plt.axvline(median_length, color='red', linestyle='--')\n",
    "\n",
    "# set the x and y labels\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# add a title\n",
    "plt.title('Distribution of sequence lengths')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = data_churn.select(['userId', 'Churn', 'page']).dropDuplicates().groupBy(['Churn', 'page']).count()\n",
    "df_col.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the window specification\n",
    "window = Window.partitionBy(\"userId\") \\\n",
    "               .orderBy(desc(\"ts\")) \\\n",
    "               .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# add a new column to the dataframe, called \"Churn\"\n",
    "# this column contains the cumulative sum of the \"Churn\" column for each user, sorted in reverse time order\n",
    "data_churn = data_churn.withColumn(\"Churn\", sum(\"Churn\").over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all columns except for userId and churn\n",
    "columns = ['gender', 'level','page']\n",
    "\n",
    "\n",
    "# create subplots\n",
    "fig, axs = plt.subplots(ncols=len(columns), figsize=(18, 6))\n",
    "\n",
    "# iterate over columns and create a bar plot for each one\n",
    "for i, col in enumerate(columns):\n",
    "    # group by column and churn and count\n",
    "    df_col = data_churn.select(['userId', 'Churn', col]).dropDuplicates().groupBy(['Churn', col]).count()\n",
    "    # collect data as a list of rows\n",
    "    rows = df_col.collect()\n",
    "    # convert rows to a list of dictionaries\n",
    "    data = [{col: r[col], 'count': r['count'], 'Churn': r['Churn']} for r in rows]\n",
    "    # set order for plotting\n",
    "    data = sorted(data, key=lambda x: x['count'], reverse=True)\n",
    "    # plot bar chart\n",
    "    ax = sns.barplot(x=col, y='count', hue='Churn', data=pd.DataFrame(data), ax=axs[i],\n",
    "                    palette=['green', 'red'])\n",
    "    # set title\n",
    "    ax.set_title(f'Number of Users That Churned by {col}')\n",
    "    # set legend\n",
    "    ax.legend(title='Churn', loc='best', labels=['Not Churned', 'Churned'])\n",
    "    # set xlabel rotation\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# set overall title\n",
    "fig.suptitle('Churn by User Attributes')\n",
    "# adjust spacing between subplots\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_page = data_churn.select(['userId', 'Churn', 'page'])\\\n",
    ".groupBy('page','Churn')\\\n",
    ".count()\\\n",
    ".toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate counts of customers who churned and those who stayed\n",
    "churn_count = group_by_page.loc[group_by_page['Churn'] == 1, 'count'].sum()\n",
    "stay_count = group_by_page.loc[group_by_page['Churn'] == 0, 'count'].sum()\n",
    "\n",
    "# calculate the rate of pages visited by those who churned vs. those who stayed\n",
    "group_by_page['rate'] = group_by_page['count'] / np.where(\n",
    "    group_by_page['Churn'] == 0, stay_count, churn_count)\n",
    "group_by_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pages by churn\n",
    "plt.figure(figsize=[10,8])\n",
    "ax = sns.scatterplot(data=group_by_page, x='rate', y='page', hue='Churn', s=200)\n",
    "\n",
    "plt.title('Rate of Pages Navigated to by Users that Churned vs. Users that Stayed')\n",
    "plt.xlabel('Rate of pages visited')\n",
    "plt.ylabel('Page')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a UDF to extract the hour from the timestamp\n",
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).hour)\n",
    "\n",
    "# create a new column for the hour\n",
    "df = data_churn.withColumn(\"hour\", get_hour(data_churn.ts))\n",
    "\n",
    "# filter the data for users who did not churn and navigated to the NextSong page\n",
    "songs_in_hour_stay = df.filter((df.page == \"NextSong\") )\n",
    "\n",
    "# group by hour and count the number of NextSong pages\n",
    "songs_in_hour_stay = songs_in_hour_stay.groupBy([\"hour\", \"Churn\"]).count()\n",
    "\n",
    "# sort the data by hour as a float\n",
    "songs_in_hour_stay = songs_in_hour_stay.orderBy(col(\"hour\").cast(\"float\"))\n",
    "\n",
    "# convert the data to a pandas dataframe\n",
    "songs_in_hour_stay_pd = songs_in_hour_stay.toPandas()\n",
    "\n",
    "# convert the hour column to numeric data type\n",
    "songs_in_hour_stay_pd[\"hour\"] = pd.to_numeric(songs_in_hour_stay_pd[\"hour\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour_stay_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the distribution\n",
    "plt.figure(figsize=[10, 8])\n",
    "sns.lineplot(data=songs_in_hour_stay_pd, x='hour', y='count', hue = \"Churn\")\n",
    "plt.xlim(-1, 24)\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Songs played\")\n",
    "plt.title(\"Number of Songs Played per Hour ( Churn & Non-Churn Users)\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat what we expect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_opsys = data_churn.select(\"userId\", \"userAgent\", \"Churn\").dropDuplicates(['userId']).toPandas()\n",
    "\n",
    "# List of browsers to extract\n",
    "browser_list = [\"Chrome\", \"Firefox\", \"Safari\", \"Trident\"]\n",
    "\n",
    "# Extract the browser from the user agent string and create a new column\n",
    "df_opsys['browser'] = df_opsys.userAgent.str.extract('(?i)({0})'.format('|'.join(browser_list)))\n",
    "\n",
    "# Group the data by OS and browser and get the count\n",
    "grouped_data = df_opsys.groupby(['Churn', 'browser']).size().unstack()\n",
    "\n",
    "# Plot a bar chart for each group\n",
    "grouped_data.plot(kind='bar', layout=(2,2), figsize=(10,8))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract registration from ts and create a new column\n",
    "days_df = data_churn.withColumn(\"diif_days\", (data_churn['ts']) - (data_churn['registration']))\n",
    "\n",
    "# Convert delta_days to days and create a new column\n",
    "days_df_pd = days_df.withColumn(\"days\", days_df['diif_days']/(1000*3600*24)).toPandas()\n",
    "\n",
    "# # Create the violin plot\n",
    "plt.figure(figsize=[8,6])\n",
    "sns.violinplot(data=days_df_pd, x='Churn', y='days')\n",
    "plt.title('Count of Users that Churned vs. Users that Stayed by Days Since Registering')\n",
    "plt.ylabel(\"Days Since Registered\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gender = data_churn\\\n",
    ".select(['userId', 'gender'])\\\n",
    ".dropDuplicates()\\\n",
    ".withColumn('gender', when(col('gender') == 'M', 1)\\\n",
    ".otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gender.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "data_level = data_churn.select(['userId', 'level', 'ts']) \\\n",
    ".dropDuplicates() \\\n",
    ".sort('userId') \\\n",
    ".withColumn(\"uptodate\", dense_rank() \\\n",
    ".over(Window.partitionBy(\"userId\") \\\n",
    ".orderBy(desc(\"ts\")))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_level.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_level_f = data_level \\\n",
    ".filter(data_level.uptodate==1) \\\n",
    ".drop('ts', 'uptodate') \\\n",
    ".withColumn('level', when(col('level') == 'paid', 1).otherwise(0))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_level_f.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_artist = data_churn\\\n",
    ".select(\"userId\", \"artist\")\\\n",
    ".dropDuplicates()\\\n",
    ".groupby(\"userId\")\\\n",
    ".count()\\\n",
    ".withColumnRenamed(\"count\", \"number_artist_listen\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_artist.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_song = data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"n_song\", when(col(\"page\") == \"NextSong\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg({'n_song': 'avg'}) \\\n",
    ".withColumnRenamed('avg(n_song)', 'nsong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_song.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Playlist = data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"p_list\", when(col(\"page\") == \"Add to Playlist\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg({'p_list': 'count'}) \\\n",
    ".withColumnRenamed('count(p_list)', 'p_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ad = data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"ad\", when(col(\"page\") == \"Roll Advert\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg({'ad': 'sum'}) \\\n",
    ".withColumnRenamed('sum(ad)', 'ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Playlist.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ad.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_likes  = data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"n_like\", when(col(\"page\") == \"Thumbs Up\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg({'n_like': 'sum'}) \\\n",
    ".withColumnRenamed('sum(n_like)', 'thumbsup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_likes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dlikes =  data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"n_dlike\", when(col(\"page\") == \"Thumbs Down\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg({'n_dlike': 'sum'}) \\\n",
    ".withColumnRenamed('sum(n_dlike)', 'thumsdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dlikes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dgrade =  data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"n_dgrade\", when(col(\"page\") == \"Downgrade\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg(sum(\"n_dgrade\").alias(\"dgrade\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dgrade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_addfriend =  data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"add_f\", when(col(\"page\") == \"Add Friend\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg(count(\"add_f\").alias(\"add_f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_addfriend.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_logout =  data_churn\\\n",
    ".select([\"userId\", \"page\"]) \\\n",
    ".withColumn(\"logout\", when(col(\"page\") == \"Logout\", 1).otherwise(0)) \\\n",
    ".groupBy(\"userId\") \\\n",
    ".agg(sum(\"logout\").alias(\"logout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_logout.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag the records where Cancellation Confirmation page is reached - 1 if it is and 0 if not\n",
    "churn_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "#creating churn column\n",
    "data_churn = data_no_duplicated.withColumn(\"Churn\", churn_event(\"page\"))\n",
    "\n",
    "# sort records for a user in reverse time order so we can add up vals in churn column\n",
    "W = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# create column churn which contains sum of churn 1s over records\n",
    "data_churn = data_churn.withColumn(\"Churn\", Fsum(\"Churn\").over(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data_churn\\\n",
    ".select(\"userId\", \"Churn\")\\\n",
    ".dropDuplicates()\\\n",
    ".groupby(\"userId\", \"Churn\")\\\n",
    ".count()\\\n",
    ".drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = data_gender\\\n",
    ".join(data_level_f, [\"userId\"])\\\n",
    ".join(data_artist, [\"userId\"])\\\n",
    ".join(data_Playlist, [\"userId\"])\\\n",
    ".join(data_likes, [\"userId\"])\\\n",
    ".join(data_dlikes, [\"userId\"])\\\n",
    ".join(data_dgrade, [\"userId\"])\\\n",
    ".join(data_song, [\"userId\"])\\\n",
    ".join(data_ad , [\"userId\"])\\\n",
    ".join(data_addfriend, [\"userId\"])\\\n",
    ".join(data_logout, [\"userId\"])\\\n",
    ".join(data_c, [\"userId\"]) \\\n",
    ".dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = [col(c).cast('float').alias(c) for c in df_ready.columns]\n",
    "\n",
    "df_ready = df_ready.select(float_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Select columns to be scaled and assemble them into a single vector column\n",
    "assembler = VectorAssembler(inputCols=['gender', 'level', 'number_artist_listen', 'p_list',\n",
    "                                      'thumbsup', 'thumsdown', 'dgrade', 'nsong', 'ad','Churn'],\n",
    "                            outputCol='features_vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled = assembler.transform(df_ready)\n",
    "\n",
    "# Fit StandardScaler model on assembled data\n",
    "scaler = StandardScaler(inputCol='features_vec', outputCol='scaled_features')\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "\n",
    "# Apply scaler model on assembled data\n",
    "df_scaled = scaler_model.transform(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled \\\n",
    "    .groupby('Churn') \\\n",
    "    .count() \\\n",
    "    .toPandas() \\\n",
    "    .value_counts() \\\n",
    "    .to_frame() \\\n",
    "    .style \\\n",
    "    .background_gradient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import ClassBalancer\n",
    "\n",
    "\n",
    "# Convert the 'Churn' column to a numeric label column\n",
    "label_indexer = StringIndexer(inputCol='Churn', outputCol='label')\n",
    "\n",
    "\n",
    "# Balance the classes\n",
    "balancer = ClassBalancer(targetRatio=1.0, labelCol='label', featuresCol='scaled_features')\n",
    "\n",
    "# Train a random forest classifier\n",
    "rf = RandomForestClassifier(featuresCol='balancedFeatures', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer,  balancer, rf])\n",
    "\n",
    "# Define the parameter grid for the random forest classifier\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "\n",
    "# Define the cross validator\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "# Fit the pipeline to the dataset\n",
    "model = cv.fit(df)\n",
    "\n",
    "# Make predictions on a test set\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Evaluate the model\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print('Area Under ROC: {:.2f}'.format(auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
